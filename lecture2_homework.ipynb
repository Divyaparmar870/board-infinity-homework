{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lecture2_homework.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMj0zk8FRMedL3Mv3RRIM7t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Divyaparmar870/board-infinity-homework/blob/master/lecture2_homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOzEv1G8W411",
        "colab_type": "text"
      },
      "source": [
        "# if and else statement "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1KZD1IsW-4I",
        "colab_type": "code",
        "outputId": "dd2c5a2a-56bb-4e6a-b607-fb2820ab11cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "age=int(input('enter your age:'))\n",
        "if age >18:\n",
        "   print('you are adult ')\n",
        "else:\n",
        "   print('you are teenage')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "enter your age:50\n",
            "you are adult \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk3NTVC_gkU6",
        "colab_type": "text"
      },
      "source": [
        "# nested if else"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt12OWN7ZhU_",
        "colab_type": "code",
        "outputId": "d93cd72b-762e-45d5-84cd-bb399b22f33e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "\n",
        "income=int(input('what is your income?'))\n",
        "ongoin_loan=str(input('do you have a on goin loan::'))\n",
        "\n",
        "if income > 100000:\n",
        "  if ongoin_loan=='yes':\n",
        "      print('sorry you are not eligble')\n",
        "  else:\n",
        "   print('congrats you are eligible')\n",
        "else:\n",
        "  print('so sorry,you are not eligible ')\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "what is your income?700\n",
            "do you have a on goin loan::yes\n",
            "so sorry,you are not eligible \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04bpTXiTeQ0i",
        "colab_type": "code",
        "outputId": "9e06c795-fd28-40e5-c599-daefef24585d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "age=str(input('are you female   '))\n",
        "if age =='yes':\n",
        " print('you are girl')\n",
        "else:\n",
        " print('you are boy')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "are you female   yes\n",
            "you are girl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2fx8-_jjKz3",
        "colab_type": "text"
      },
      "source": [
        "# for loops\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nt6kBMBVjPzK",
        "colab_type": "code",
        "outputId": "e8ba2c77-8b89-4cba-b70d-4e1d2e6f930f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "number=1,2,3,45,5,6,7,8\n",
        "for X in number:\n",
        "  print(X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "45\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvg5-yRpkLbw",
        "colab_type": "text"
      },
      "source": [
        "# print even numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l2Gz4J3jfOg",
        "colab_type": "code",
        "outputId": "6ae1d60e-3f96-49b0-e186-4dfdedb78f41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "number=1,2,3,4,6,7,8,9,100\n",
        "for x in number:\n",
        "  if x%2==0:\n",
        "    print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "4\n",
            "6\n",
            "8\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14mQssCAkWoI",
        "colab_type": "text"
      },
      "source": [
        "# 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIrkV5w2kZqO",
        "colab_type": "code",
        "outputId": "93053163-3156-4ee9-f4fd-5467162115b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "myself={'name':'divya','age':'23','gender':'female','favfood':'lollypop'}\n",
        "for x in myself:\n",
        "\n",
        "  def print_keys(my_dictionary):\n",
        "   return my_dictionary.keys()\n",
        "print(print_keys(myself))\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['name', 'age', 'gender', 'favfood'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8_lrcdUn8bY",
        "colab_type": "code",
        "outputId": "c41b9169-bae6-4a95-b264-eced6236acdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "for x in myself:\n",
        "  def print_values(my_dictionary):\n",
        "   return my_dictionary.values()\n",
        "print(print_values(myself))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_values(['divya', '23', 'female', 'lollypop'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX87Gm_AtbQS",
        "colab_type": "code",
        "outputId": "40d321a3-b517-483e-a1c8-75ea2e1f83fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "def my_func(person_name):\n",
        "   return person_name\n",
        "print(my_func('divya'))\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "divya\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzOajzErPag3",
        "colab_type": "text"
      },
      "source": [
        "# create a function to print only keys from the dictionary \n",
        "\n",
        "-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFIQg7r4PRnS",
        "colab_type": "code",
        "outputId": "6483ad0f-72ee-4823-acb7-f24bf6cda61d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "me={'name':'divya','age':'23','gender':'female','favfood':'lollypop'}\n",
        "for x in me:\n",
        "\n",
        "  def print_keys(my_dictionary):\n",
        "   return my_dictionary.keys()\n",
        "print(print_keys(me))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['name', 'age', 'gender', 'favfood'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc6gnz4JKzKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BixYJfhkQUsH",
        "colab_type": "text"
      },
      "source": [
        "# Create a function that takes a list of 10 random numbers and prints out only odd numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITdfGPHPTimL",
        "colab_type": "code",
        "outputId": "580b8b61-8f4c-4cd3-fb4d-49defe0e9eaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "list3=[1,2,3,4,5,6,7,8,9,11]\n",
        "def my_func(list3):\n",
        "  return list3\n",
        "  print(my_func(list3))\n",
        "for x in list3:\n",
        "  if x%2 !=0:\n",
        "    print(x) \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "3\n",
            "5\n",
            "7\n",
            "9\n",
            "11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUdo7CF5VtfK",
        "colab_type": "text"
      },
      "source": [
        "# Type conversion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFeVL4fBNN-_",
        "colab_type": "text"
      },
      "source": [
        "# integer to float"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvmZuLyKK0Fe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "number=56\n",
        "number=float(number)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4hZTKCzMw9V",
        "colab_type": "code",
        "outputId": "09f157e8-f6cb-4478-8716-a58996f35443",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(number)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "float"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn1DX_0xYGo4",
        "colab_type": "code",
        "outputId": "7b34391b-8a75-4f9c-954a-8af453d770eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "float(number)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc6upMaTNXsk",
        "colab_type": "text"
      },
      "source": [
        "# string to integer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmklN6qKNb9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "myname='123'\n",
        "myname=int(myname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx5cxyKhNqu_",
        "colab_type": "code",
        "outputId": "ad49bd7a-4611-43bf-ceca-441042e12419",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(myname)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "int"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJweEJFdX-rM",
        "colab_type": "code",
        "outputId": "9b119026-92af-4284-a2b0-14fd6027a447",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "int(myname)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "123"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNu31AgwNvMp",
        "colab_type": "text"
      },
      "source": [
        "# integer to string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdPn0nCtN3ov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "swap=1238\n",
        "swap=str(swap)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfN2NGdtOAOs",
        "colab_type": "code",
        "outputId": "c56c7586-9035-4b27-a27f-74c625763261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(swap)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii0WP_lAYMVV",
        "colab_type": "code",
        "outputId": "9b00d62f-0213-4a17-e389-314dd13769eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "str(swap)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1238'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kmmablvPlvO",
        "colab_type": "text"
      },
      "source": [
        "# boolean to string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG3iOgIIPtOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "female=True \n",
        "female=str(female)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBwgu1KdP0Al",
        "colab_type": "code",
        "outputId": "2bb506da-36ed-4dc2-decf-41fc9e883096",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(female)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U0nMVipYQrP",
        "colab_type": "code",
        "outputId": "17745906-0c8e-4c05-b6db-d067147a2545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "str(female)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'True'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ALF3-hTWEh_",
        "colab_type": "text"
      },
      "source": [
        "# Class inheritance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXTx5xPl8vU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKdF7uUr9LNo",
        "colab_type": "text"
      },
      "source": [
        "Then import the following\n",
        "\n",
        "1. Random forest\n",
        "2. Gaussian Naive bayes\n",
        "3. Decision tree\n",
        "4. Linear regression\n",
        "5. KMeans\n",
        "6. Train test split\n",
        "7. Classification report\n",
        "8. Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4xC_SuN9gp3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9b16a931-7396-4702-82bf-91e8613af5b8"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "help(RandomForestClassifier)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class RandomForestClassifier in module sklearn.ensemble._forest:\n",
            "\n",
            "class RandomForestClassifier(ForestClassifier)\n",
            " |  A random forest classifier.\n",
            " |  \n",
            " |  A random forest is a meta estimator that fits a number of decision tree\n",
            " |  classifiers on various sub-samples of the dataset and uses averaging to\n",
            " |  improve the predictive accuracy and control over-fitting.\n",
            " |  The sub-sample size is always the same as the original\n",
            " |  input sample size but the samples are drawn with replacement if\n",
            " |  `bootstrap=True` (default).\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <forest>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  n_estimators : integer, optional (default=100)\n",
            " |      The number of trees in the forest.\n",
            " |  \n",
            " |      .. versionchanged:: 0.22\n",
            " |         The default value of ``n_estimators`` changed from 10 to 100\n",
            " |         in 0.22.\n",
            " |  \n",
            " |  criterion : string, optional (default=\"gini\")\n",
            " |      The function to measure the quality of a split. Supported criteria are\n",
            " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
            " |      Note: this parameter is tree-specific.\n",
            " |  \n",
            " |  max_depth : integer or None, optional (default=None)\n",
            " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
            " |      all leaves are pure or until all leaves contain less than\n",
            " |      min_samples_split samples.\n",
            " |  \n",
            " |  min_samples_split : int, float, optional (default=2)\n",
            " |      The minimum number of samples required to split an internal node:\n",
            " |  \n",
            " |      - If int, then consider `min_samples_split` as the minimum number.\n",
            " |      - If float, then `min_samples_split` is a fraction and\n",
            " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
            " |        number of samples for each split.\n",
            " |  \n",
            " |      .. versionchanged:: 0.18\n",
            " |         Added float values for fractions.\n",
            " |  \n",
            " |  min_samples_leaf : int, float, optional (default=1)\n",
            " |      The minimum number of samples required to be at a leaf node.\n",
            " |      A split point at any depth will only be considered if it leaves at\n",
            " |      least ``min_samples_leaf`` training samples in each of the left and\n",
            " |      right branches.  This may have the effect of smoothing the model,\n",
            " |      especially in regression.\n",
            " |  \n",
            " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
            " |      - If float, then `min_samples_leaf` is a fraction and\n",
            " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
            " |        number of samples for each node.\n",
            " |  \n",
            " |      .. versionchanged:: 0.18\n",
            " |         Added float values for fractions.\n",
            " |  \n",
            " |  min_weight_fraction_leaf : float, optional (default=0.)\n",
            " |      The minimum weighted fraction of the sum total of weights (of all\n",
            " |      the input samples) required to be at a leaf node. Samples have\n",
            " |      equal weight when sample_weight is not provided.\n",
            " |  \n",
            " |  max_features : int, float, string or None, optional (default=\"auto\")\n",
            " |      The number of features to consider when looking for the best split:\n",
            " |  \n",
            " |      - If int, then consider `max_features` features at each split.\n",
            " |      - If float, then `max_features` is a fraction and\n",
            " |        `int(max_features * n_features)` features are considered at each\n",
            " |        split.\n",
            " |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
            " |      - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
            " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
            " |      - If None, then `max_features=n_features`.\n",
            " |  \n",
            " |      Note: the search for a split does not stop until at least one\n",
            " |      valid partition of the node samples is found, even if it requires to\n",
            " |      effectively inspect more than ``max_features`` features.\n",
            " |  \n",
            " |  max_leaf_nodes : int or None, optional (default=None)\n",
            " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
            " |      Best nodes are defined as relative reduction in impurity.\n",
            " |      If None then unlimited number of leaf nodes.\n",
            " |  \n",
            " |  min_impurity_decrease : float, optional (default=0.)\n",
            " |      A node will be split if this split induces a decrease of the impurity\n",
            " |      greater than or equal to this value.\n",
            " |  \n",
            " |      The weighted impurity decrease equation is the following::\n",
            " |  \n",
            " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
            " |                              - N_t_L / N_t * left_impurity)\n",
            " |  \n",
            " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
            " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
            " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
            " |  \n",
            " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
            " |      if ``sample_weight`` is passed.\n",
            " |  \n",
            " |      .. versionadded:: 0.19\n",
            " |  \n",
            " |  min_impurity_split : float, (default=1e-7)\n",
            " |      Threshold for early stopping in tree growth. A node will split\n",
            " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
            " |  \n",
            " |      .. deprecated:: 0.19\n",
            " |         ``min_impurity_split`` has been deprecated in favor of\n",
            " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
            " |         ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
            " |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
            " |  \n",
            " |  \n",
            " |  bootstrap : boolean, optional (default=True)\n",
            " |      Whether bootstrap samples are used when building trees. If False, the\n",
            " |      whole datset is used to build each tree.\n",
            " |  \n",
            " |  oob_score : bool (default=False)\n",
            " |      Whether to use out-of-bag samples to estimate\n",
            " |      the generalization accuracy.\n",
            " |  \n",
            " |  n_jobs : int or None, optional (default=None)\n",
            " |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
            " |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
            " |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
            " |      context. ``-1`` means using all processors. See :term:`Glossary\n",
            " |      <n_jobs>` for more details.\n",
            " |  \n",
            " |  random_state : int, RandomState instance or None, optional (default=None)\n",
            " |      Controls both the randomness of the bootstrapping of the samples used\n",
            " |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
            " |      features to consider when looking for the best split at each node\n",
            " |      (if ``max_features < n_features``).\n",
            " |      See :term:`Glossary <random_state>` for details.\n",
            " |  \n",
            " |  verbose : int, optional (default=0)\n",
            " |      Controls the verbosity when fitting and predicting.\n",
            " |  \n",
            " |  warm_start : bool, optional (default=False)\n",
            " |      When set to ``True``, reuse the solution of the previous call to fit\n",
            " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
            " |      new forest. See :term:`the Glossary <warm_start>`.\n",
            " |  \n",
            " |  class_weight : dict, list of dicts, \"balanced\", \"balanced_subsample\" or     None, optional (default=None)\n",
            " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
            " |      If not given, all classes are supposed to have weight one. For\n",
            " |      multi-output problems, a list of dicts can be provided in the same\n",
            " |      order as the columns of y.\n",
            " |  \n",
            " |      Note that for multioutput (including multilabel) weights should be\n",
            " |      defined for each class of every column in its own dict. For example,\n",
            " |      for four-class multilabel classification weights should be\n",
            " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
            " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
            " |  \n",
            " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
            " |      weights inversely proportional to class frequencies in the input data\n",
            " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
            " |  \n",
            " |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
            " |      weights are computed based on the bootstrap sample for every tree\n",
            " |      grown.\n",
            " |  \n",
            " |      For multi-output, the weights of each column of y will be multiplied.\n",
            " |  \n",
            " |      Note that these weights will be multiplied with sample_weight (passed\n",
            " |      through the fit method) if sample_weight is specified.\n",
            " |  \n",
            " |  ccp_alpha : non-negative float, optional (default=0.0)\n",
            " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
            " |      subtree with the largest cost complexity that is smaller than\n",
            " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
            " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
            " |  \n",
            " |      .. versionadded:: 0.22\n",
            " |  \n",
            " |  max_samples : int or float, default=None\n",
            " |      If bootstrap is True, the number of samples to draw from X\n",
            " |      to train each base estimator.\n",
            " |  \n",
            " |      - If None (default), then draw `X.shape[0]` samples.\n",
            " |      - If int, then draw `max_samples` samples.\n",
            " |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
            " |        `max_samples` should be in the interval `(0, 1)`.\n",
            " |  \n",
            " |      .. versionadded:: 0.22\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  base_estimator_ : DecisionTreeClassifier\n",
            " |      The child estimator template used to create the collection of fitted\n",
            " |      sub-estimators.\n",
            " |  \n",
            " |  estimators_ : list of DecisionTreeClassifier\n",
            " |      The collection of fitted sub-estimators.\n",
            " |  \n",
            " |  classes_ : array of shape (n_classes,) or a list of such arrays\n",
            " |      The classes labels (single output problem), or a list of arrays of\n",
            " |      class labels (multi-output problem).\n",
            " |  \n",
            " |  n_classes_ : int or list\n",
            " |      The number of classes (single output problem), or a list containing the\n",
            " |      number of classes for each output (multi-output problem).\n",
            " |  \n",
            " |  n_features_ : int\n",
            " |      The number of features when ``fit`` is performed.\n",
            " |  \n",
            " |  n_outputs_ : int\n",
            " |      The number of outputs when ``fit`` is performed.\n",
            " |  \n",
            " |  feature_importances_ : ndarray of shape (n_features,)\n",
            " |      The feature importances (the higher, the more important the feature).\n",
            " |  \n",
            " |  oob_score_ : float\n",
            " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
            " |      This attribute exists only when ``oob_score`` is True.\n",
            " |  \n",
            " |  oob_decision_function_ : array of shape (n_samples, n_classes)\n",
            " |      Decision function computed with out-of-bag estimate on the training\n",
            " |      set. If n_estimators is small it might be possible that a data point\n",
            " |      was never left out during the bootstrap. In this case,\n",
            " |      `oob_decision_function_` might contain NaN. This attribute exists\n",
            " |      only when ``oob_score`` is True.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from sklearn.ensemble import RandomForestClassifier\n",
            " |  >>> from sklearn.datasets import make_classification\n",
            " |  \n",
            " |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
            " |  ...                            n_informative=2, n_redundant=0,\n",
            " |  ...                            random_state=0, shuffle=False)\n",
            " |  >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
            " |  >>> clf.fit(X, y)\n",
            " |  RandomForestClassifier(max_depth=2, random_state=0)\n",
            " |  >>> print(clf.feature_importances_)\n",
            " |  [0.14205973 0.76664038 0.0282433  0.06305659]\n",
            " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
            " |  [1]\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  The default values for the parameters controlling the size of the trees\n",
            " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
            " |  unpruned trees which can potentially be very large on some data sets. To\n",
            " |  reduce memory consumption, the complexity and size of the trees should be\n",
            " |  controlled by setting those parameter values.\n",
            " |  \n",
            " |  The features are always randomly permuted at each split. Therefore,\n",
            " |  the best found split may vary, even with the same training data,\n",
            " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
            " |  of the criterion is identical for several splits enumerated during the\n",
            " |  search of the best split. To obtain a deterministic behaviour during\n",
            " |  fitting, ``random_state`` has to be fixed.\n",
            " |  \n",
            " |  References\n",
            " |  ----------\n",
            " |  \n",
            " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  DecisionTreeClassifier, ExtraTreesClassifier\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      RandomForestClassifier\n",
            " |      ForestClassifier\n",
            " |      sklearn.base.ClassifierMixin\n",
            " |      BaseForest\n",
            " |      sklearn.base.MultiOutputMixin\n",
            " |      sklearn.ensemble._base.BaseEnsemble\n",
            " |      sklearn.base.MetaEstimatorMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from ForestClassifier:\n",
            " |  \n",
            " |  predict(self, X)\n",
            " |      Predict class for X.\n",
            " |      \n",
            " |      The predicted class of an input sample is a vote by the trees in\n",
            " |      the forest, weighted by their probability estimates. That is,\n",
            " |      the predicted class is the one with highest mean probability\n",
            " |      estimate across the trees.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like or sparse matrix of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, its dtype will be converted to\n",
            " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            " |          converted into a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          The predicted classes.\n",
            " |  \n",
            " |  predict_log_proba(self, X)\n",
            " |      Predict class log-probabilities for X.\n",
            " |      \n",
            " |      The predicted class log-probabilities of an input sample is computed as\n",
            " |      the log of the mean predicted class probabilities of the trees in the\n",
            " |      forest.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like or sparse matrix of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, its dtype will be converted to\n",
            " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            " |          converted into a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      p : array of shape (n_samples, n_classes), or a list of n_outputs\n",
            " |          such arrays if n_outputs > 1.\n",
            " |          The class probabilities of the input samples. The order of the\n",
            " |          classes corresponds to that in the attribute :term:`classes_`.\n",
            " |  \n",
            " |  predict_proba(self, X)\n",
            " |      Predict class probabilities for X.\n",
            " |      \n",
            " |      The predicted class probabilities of an input sample are computed as\n",
            " |      the mean predicted class probabilities of the trees in the forest.\n",
            " |      The class probability of a single tree is the fraction of samples of\n",
            " |      the same class in a leaf.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like or sparse matrix of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, its dtype will be converted to\n",
            " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            " |          converted into a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      p : array of shape (n_samples, n_classes), or a list of n_outputs\n",
            " |          such arrays if n_outputs > 1.\n",
            " |          The class probabilities of the input samples. The order of the\n",
            " |          classes corresponds to that in the attribute :term:`classes_`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the mean accuracy on the given test data and labels.\n",
            " |      \n",
            " |      In multi-label classification, this is the subset accuracy\n",
            " |      which is a harsh metric since you require for each sample that\n",
            " |      each label set be correctly predicted.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True labels for X.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          Mean accuracy of self.predict(X) wrt. y.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from BaseForest:\n",
            " |  \n",
            " |  apply(self, X)\n",
            " |      Apply trees in the forest to X, return leaf indices.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like or sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, its dtype will be converted to\n",
            " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            " |          converted into a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
            " |          For each datapoint x in X and for each tree in the forest,\n",
            " |          return the index of the leaf x ends up in.\n",
            " |  \n",
            " |  decision_path(self, X)\n",
            " |      Return the decision path in the forest.\n",
            " |      \n",
            " |      .. versionadded:: 0.18\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like or sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, its dtype will be converted to\n",
            " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            " |          converted into a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
            " |          Return a node indicator matrix where non zero elements\n",
            " |          indicates that the samples goes through the nodes.\n",
            " |      \n",
            " |      n_nodes_ptr : array of size (n_estimators + 1, )\n",
            " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
            " |          gives the indicator value for the i-th estimator.\n",
            " |  \n",
            " |  fit(self, X, y, sample_weight=None)\n",
            " |      Build a forest of trees from the training set (X, y).\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like or sparse matrix of shape (n_samples, n_features)\n",
            " |          The training input samples. Internally, its dtype will be converted\n",
            " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            " |          converted into a sparse ``csc_matrix``.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          The target values (class labels in classification, real numbers in\n",
            " |          regression).\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights. If None, then samples are equally weighted. Splits\n",
            " |          that would create child nodes with net zero or negative weight are\n",
            " |          ignored while searching for a split in each node. In the case of\n",
            " |          classification, splits are also ignored if they would result in any\n",
            " |          single class carrying a negative weight in either child node.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from BaseForest:\n",
            " |  \n",
            " |  feature_importances_\n",
            " |      Return the feature importances (the higher, the more important the\n",
            " |         feature).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      feature_importances_ : array, shape = [n_features]\n",
            " |          The values of this array sum to 1, unless all trees are single node\n",
            " |          trees consisting of only the root node, in which case it will be an\n",
            " |          array of zeros.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
            " |  \n",
            " |  __getitem__(self, index)\n",
            " |      Return the index'th estimator in the ensemble.\n",
            " |  \n",
            " |  __iter__(self)\n",
            " |      Return iterator over estimators in the ensemble.\n",
            " |  \n",
            " |  __len__(self)\n",
            " |      Return the number of estimators in the ensemble.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : mapping of string to any\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as pipelines). The latter have parameters of the form\n",
            " |      ``<component>__<parameter>`` so that it's possible to update each\n",
            " |      component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Estimator instance.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4_vQZcN-H-6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "95bfd1cd-749b-4004-b53c-c95a9a7a420a"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "help(GaussianNB)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class GaussianNB in module sklearn.naive_bayes:\n",
            "\n",
            "class GaussianNB(_BaseNB)\n",
            " |  Gaussian Naive Bayes (GaussianNB)\n",
            " |  \n",
            " |  Can perform online updates to model parameters via :meth:`partial_fit`.\n",
            " |  For details on algorithm used to update feature means and variance online,\n",
            " |  see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n",
            " |  \n",
            " |      http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <gaussian_naive_bayes>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  priors : array-like, shape (n_classes,)\n",
            " |      Prior probabilities of the classes. If specified the priors are not\n",
            " |      adjusted according to the data.\n",
            " |  \n",
            " |  var_smoothing : float, optional (default=1e-9)\n",
            " |      Portion of the largest variance of all features that is added to\n",
            " |      variances for calculation stability.\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  class_count_ : array, shape (n_classes,)\n",
            " |      number of training samples observed in each class.\n",
            " |  \n",
            " |  class_prior_ : array, shape (n_classes,)\n",
            " |      probability of each class.\n",
            " |  \n",
            " |  classes_ : array, shape (n_classes,)\n",
            " |      class labels known to the classifier\n",
            " |  \n",
            " |  epsilon_ : float\n",
            " |      absolute additive value to variances\n",
            " |  \n",
            " |  sigma_ : array, shape (n_classes, n_features)\n",
            " |      variance of each feature per class\n",
            " |  \n",
            " |  theta_ : array, shape (n_classes, n_features)\n",
            " |      mean of each feature per class\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> import numpy as np\n",
            " |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
            " |  >>> Y = np.array([1, 1, 1, 2, 2, 2])\n",
            " |  >>> from sklearn.naive_bayes import GaussianNB\n",
            " |  >>> clf = GaussianNB()\n",
            " |  >>> clf.fit(X, Y)\n",
            " |  GaussianNB()\n",
            " |  >>> print(clf.predict([[-0.8, -1]]))\n",
            " |  [1]\n",
            " |  >>> clf_pf = GaussianNB()\n",
            " |  >>> clf_pf.partial_fit(X, Y, np.unique(Y))\n",
            " |  GaussianNB()\n",
            " |  >>> print(clf_pf.predict([[-0.8, -1]]))\n",
            " |  [1]\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      GaussianNB\n",
            " |      _BaseNB\n",
            " |      sklearn.base.ClassifierMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, priors=None, var_smoothing=1e-09)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, X, y, sample_weight=None)\n",
            " |      Fit Gaussian Naive Bayes according to X, y\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like, shape (n_samples, n_features)\n",
            " |          Training vectors, where n_samples is the number of samples\n",
            " |          and n_features is the number of features.\n",
            " |      \n",
            " |      y : array-like, shape (n_samples,)\n",
            " |          Target values.\n",
            " |      \n",
            " |      sample_weight : array-like, shape (n_samples,), optional (default=None)\n",
            " |          Weights applied to individual samples (1. for unweighted).\n",
            " |      \n",
            " |          .. versionadded:: 0.17\n",
            " |             Gaussian Naive Bayes supports fitting with *sample_weight*.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |  \n",
            " |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
            " |      Incremental fit on a batch of samples.\n",
            " |      \n",
            " |      This method is expected to be called several times consecutively\n",
            " |      on different chunks of a dataset so as to implement out-of-core\n",
            " |      or online learning.\n",
            " |      \n",
            " |      This is especially useful when the whole dataset is too big to fit in\n",
            " |      memory at once.\n",
            " |      \n",
            " |      This method has some performance and numerical stability overhead,\n",
            " |      hence it is better to call partial_fit on chunks of data that are\n",
            " |      as large as possible (as long as fitting in the memory budget) to\n",
            " |      hide the overhead.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like, shape (n_samples, n_features)\n",
            " |          Training vectors, where n_samples is the number of samples and\n",
            " |          n_features is the number of features.\n",
            " |      \n",
            " |      y : array-like, shape (n_samples,)\n",
            " |          Target values.\n",
            " |      \n",
            " |      classes : array-like, shape (n_classes,), optional (default=None)\n",
            " |          List of all the classes that can possibly appear in the y vector.\n",
            " |      \n",
            " |          Must be provided at the first call to partial_fit, can be omitted\n",
            " |          in subsequent calls.\n",
            " |      \n",
            " |      sample_weight : array-like, shape (n_samples,), optional (default=None)\n",
            " |          Weights applied to individual samples (1. for unweighted).\n",
            " |      \n",
            " |          .. versionadded:: 0.17\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from _BaseNB:\n",
            " |  \n",
            " |  predict(self, X)\n",
            " |      Perform classification on an array of test vectors X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      C : ndarray of shape (n_samples,)\n",
            " |          Predicted target values for X\n",
            " |  \n",
            " |  predict_log_proba(self, X)\n",
            " |      Return log-probability estimates for the test vector X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      C : array-like of shape (n_samples, n_classes)\n",
            " |          Returns the log-probability of the samples for each class in\n",
            " |          the model. The columns correspond to the classes in sorted\n",
            " |          order, as they appear in the attribute :term:`classes_`.\n",
            " |  \n",
            " |  predict_proba(self, X)\n",
            " |      Return probability estimates for the test vector X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      C : array-like of shape (n_samples, n_classes)\n",
            " |          Returns the probability of the samples for each class in\n",
            " |          the model. The columns correspond to the classes in sorted\n",
            " |          order, as they appear in the attribute :term:`classes_`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the mean accuracy on the given test data and labels.\n",
            " |      \n",
            " |      In multi-label classification, this is the subset accuracy\n",
            " |      which is a harsh metric since you require for each sample that\n",
            " |      each label set be correctly predicted.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True labels for X.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          Mean accuracy of self.predict(X) wrt. y.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : mapping of string to any\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as pipelines). The latter have parameters of the form\n",
            " |      ``<component>__<parameter>`` so that it's possible to update each\n",
            " |      component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Estimator instance.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV5QDEmi-w0o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bdeafadd-72b4-470e-8d32-7238fdd315e0"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "help(DecisionTreeClassifier)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class DecisionTreeClassifier in module sklearn.tree._classes:\n",
            "\n",
            "class DecisionTreeClassifier(sklearn.base.ClassifierMixin, BaseDecisionTree)\n",
            " |  A decision tree classifier.\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <tree>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
            " |      The function to measure the quality of a split. Supported criteria are\n",
            " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
            " |  \n",
            " |  splitter : {\"best\", \"random\"}, default=\"best\"\n",
            " |      The strategy used to choose the split at each node. Supported\n",
            " |      strategies are \"best\" to choose the best split and \"random\" to choose\n",
            " |      the best random split.\n",
            " |  \n",
            " |  max_depth : int, default=None\n",
            " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
            " |      all leaves are pure or until all leaves contain less than\n",
            " |      min_samples_split samples.\n",
            " |  \n",
            " |  min_samples_split : int or float, default=2\n",
            " |      The minimum number of samples required to split an internal node:\n",
            " |  \n",
            " |      - If int, then consider `min_samples_split` as the minimum number.\n",
            " |      - If float, then `min_samples_split` is a fraction and\n",
            " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
            " |        number of samples for each split.\n",
            " |  \n",
            " |      .. versionchanged:: 0.18\n",
            " |         Added float values for fractions.\n",
            " |  \n",
            " |  min_samples_leaf : int or float, default=1\n",
            " |      The minimum number of samples required to be at a leaf node.\n",
            " |      A split point at any depth will only be considered if it leaves at\n",
            " |      least ``min_samples_leaf`` training samples in each of the left and\n",
            " |      right branches.  This may have the effect of smoothing the model,\n",
            " |      especially in regression.\n",
            " |  \n",
            " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
            " |      - If float, then `min_samples_leaf` is a fraction and\n",
            " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
            " |        number of samples for each node.\n",
            " |  \n",
            " |      .. versionchanged:: 0.18\n",
            " |         Added float values for fractions.\n",
            " |  \n",
            " |  min_weight_fraction_leaf : float, default=0.0\n",
            " |      The minimum weighted fraction of the sum total of weights (of all\n",
            " |      the input samples) required to be at a leaf node. Samples have\n",
            " |      equal weight when sample_weight is not provided.\n",
            " |  \n",
            " |  max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
            " |      The number of features to consider when looking for the best split:\n",
            " |  \n",
            " |          - If int, then consider `max_features` features at each split.\n",
            " |          - If float, then `max_features` is a fraction and\n",
            " |            `int(max_features * n_features)` features are considered at each\n",
            " |            split.\n",
            " |          - If \"auto\", then `max_features=sqrt(n_features)`.\n",
            " |          - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
            " |          - If \"log2\", then `max_features=log2(n_features)`.\n",
            " |          - If None, then `max_features=n_features`.\n",
            " |  \n",
            " |      Note: the search for a split does not stop until at least one\n",
            " |      valid partition of the node samples is found, even if it requires to\n",
            " |      effectively inspect more than ``max_features`` features.\n",
            " |  \n",
            " |  random_state : int or RandomState, default=None\n",
            " |      If int, random_state is the seed used by the random number generator;\n",
            " |      If RandomState instance, random_state is the random number generator;\n",
            " |      If None, the random number generator is the RandomState instance used\n",
            " |      by `np.random`.\n",
            " |  \n",
            " |  max_leaf_nodes : int, default=None\n",
            " |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
            " |      Best nodes are defined as relative reduction in impurity.\n",
            " |      If None then unlimited number of leaf nodes.\n",
            " |  \n",
            " |  min_impurity_decrease : float, default=0.0\n",
            " |      A node will be split if this split induces a decrease of the impurity\n",
            " |      greater than or equal to this value.\n",
            " |  \n",
            " |      The weighted impurity decrease equation is the following::\n",
            " |  \n",
            " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
            " |                              - N_t_L / N_t * left_impurity)\n",
            " |  \n",
            " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
            " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
            " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
            " |  \n",
            " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
            " |      if ``sample_weight`` is passed.\n",
            " |  \n",
            " |      .. versionadded:: 0.19\n",
            " |  \n",
            " |  min_impurity_split : float, default=1e-7\n",
            " |      Threshold for early stopping in tree growth. A node will split\n",
            " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
            " |  \n",
            " |      .. deprecated:: 0.19\n",
            " |         ``min_impurity_split`` has been deprecated in favor of\n",
            " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
            " |         ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
            " |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
            " |  \n",
            " |  class_weight : dict, list of dict or \"balanced\", default=None\n",
            " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
            " |      If None, all classes are supposed to have weight one. For\n",
            " |      multi-output problems, a list of dicts can be provided in the same\n",
            " |      order as the columns of y.\n",
            " |  \n",
            " |      Note that for multioutput (including multilabel) weights should be\n",
            " |      defined for each class of every column in its own dict. For example,\n",
            " |      for four-class multilabel classification weights should be\n",
            " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
            " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
            " |  \n",
            " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
            " |      weights inversely proportional to class frequencies in the input data\n",
            " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
            " |  \n",
            " |      For multi-output, the weights of each column of y will be multiplied.\n",
            " |  \n",
            " |      Note that these weights will be multiplied with sample_weight (passed\n",
            " |      through the fit method) if sample_weight is specified.\n",
            " |  \n",
            " |  presort : deprecated, default='deprecated'\n",
            " |      This parameter is deprecated and will be removed in v0.24.\n",
            " |  \n",
            " |      .. deprecated:: 0.22\n",
            " |  \n",
            " |  ccp_alpha : non-negative float, default=0.0\n",
            " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
            " |      subtree with the largest cost complexity that is smaller than\n",
            " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
            " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
            " |  \n",
            " |      .. versionadded:: 0.22\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  classes_ : ndarray of shape (n_classes,) or list of ndarray\n",
            " |      The classes labels (single output problem),\n",
            " |      or a list of arrays of class labels (multi-output problem).\n",
            " |  \n",
            " |  feature_importances_ : ndarray of shape (n_features,)\n",
            " |      The feature importances. The higher, the more important the\n",
            " |      feature. The importance of a feature is computed as the (normalized)\n",
            " |      total reduction of the criterion brought by that feature.  It is also\n",
            " |      known as the Gini importance [4]_.\n",
            " |  \n",
            " |  max_features_ : int\n",
            " |      The inferred value of max_features.\n",
            " |  \n",
            " |  n_classes_ : int or list of int\n",
            " |      The number of classes (for single output problems),\n",
            " |      or a list containing the number of classes for each\n",
            " |      output (for multi-output problems).\n",
            " |  \n",
            " |  n_features_ : int\n",
            " |      The number of features when ``fit`` is performed.\n",
            " |  \n",
            " |  n_outputs_ : int\n",
            " |      The number of outputs when ``fit`` is performed.\n",
            " |  \n",
            " |  tree_ : Tree\n",
            " |      The underlying Tree object. Please refer to\n",
            " |      ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
            " |      :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
            " |      for basic usage of these attributes.\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  DecisionTreeRegressor : A decision tree regressor.\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  The default values for the parameters controlling the size of the trees\n",
            " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
            " |  unpruned trees which can potentially be very large on some data sets. To\n",
            " |  reduce memory consumption, the complexity and size of the trees should be\n",
            " |  controlled by setting those parameter values.\n",
            " |  \n",
            " |  The features are always randomly permuted at each split. Therefore,\n",
            " |  the best found split may vary, even with the same training data and\n",
            " |  ``max_features=n_features``, if the improvement of the criterion is\n",
            " |  identical for several splits enumerated during the search of the best\n",
            " |  split. To obtain a deterministic behaviour during fitting,\n",
            " |  ``random_state`` has to be fixed.\n",
            " |  \n",
            " |  References\n",
            " |  ----------\n",
            " |  \n",
            " |  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
            " |  \n",
            " |  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
            " |         and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
            " |  \n",
            " |  .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
            " |         Learning\", Springer, 2009.\n",
            " |  \n",
            " |  .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
            " |         https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from sklearn.datasets import load_iris\n",
            " |  >>> from sklearn.model_selection import cross_val_score\n",
            " |  >>> from sklearn.tree import DecisionTreeClassifier\n",
            " |  >>> clf = DecisionTreeClassifier(random_state=0)\n",
            " |  >>> iris = load_iris()\n",
            " |  >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
            " |  ...                             # doctest: +SKIP\n",
            " |  ...\n",
            " |  array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n",
            " |          0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      DecisionTreeClassifier\n",
            " |      sklearn.base.ClassifierMixin\n",
            " |      BaseDecisionTree\n",
            " |      sklearn.base.MultiOutputMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted=None)\n",
            " |      Build a decision tree classifier from the training set (X, y).\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The training input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csc_matrix``.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          The target values (class labels) as integers or strings.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights. If None, then samples are equally weighted. Splits\n",
            " |          that would create child nodes with net zero or negative weight are\n",
            " |          ignored while searching for a split in each node. Splits are also\n",
            " |          ignored if they would result in any single class carrying a\n",
            " |          negative weight in either child node.\n",
            " |      \n",
            " |      check_input : bool, default=True\n",
            " |          Allow to bypass several input checking.\n",
            " |          Don't use this parameter unless you know what you do.\n",
            " |      \n",
            " |      X_idx_sorted : array-like of shape (n_samples, n_features),                 default=None\n",
            " |          The indexes of the sorted training input samples. If many tree\n",
            " |          are grown on the same dataset, this allows the ordering to be\n",
            " |          cached between trees. If None, the data will be sorted here.\n",
            " |          Don't use this parameter unless you know what to do.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : DecisionTreeClassifier\n",
            " |          Fitted estimator.\n",
            " |  \n",
            " |  predict_log_proba(self, X)\n",
            " |      Predict class log-probabilities of the input samples X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
            " |          The class log-probabilities of the input samples. The order of the\n",
            " |          classes corresponds to that in the attribute :term:`classes_`.\n",
            " |  \n",
            " |  predict_proba(self, X, check_input=True)\n",
            " |      Predict class probabilities of the input samples X.\n",
            " |      \n",
            " |      The predicted class probability is the fraction of samples of the same\n",
            " |      class in a leaf.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      check_input : bool, default=True\n",
            " |          Allow to bypass several input checking.\n",
            " |          Don't use this parameter unless you know what you do.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
            " |          The class probabilities of the input samples. The order of the\n",
            " |          classes corresponds to that in the attribute :term:`classes_`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the mean accuracy on the given test data and labels.\n",
            " |      \n",
            " |      In multi-label classification, this is the subset accuracy\n",
            " |      which is a harsh metric since you require for each sample that\n",
            " |      each label set be correctly predicted.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True labels for X.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          Mean accuracy of self.predict(X) wrt. y.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from BaseDecisionTree:\n",
            " |  \n",
            " |  apply(self, X, check_input=True)\n",
            " |      Return the index of the leaf that each sample is predicted as.\n",
            " |      \n",
            " |      .. versionadded:: 0.17\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      check_input : bool, default=True\n",
            " |          Allow to bypass several input checking.\n",
            " |          Don't use this parameter unless you know what you do.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X_leaves : array-like of shape (n_samples,)\n",
            " |          For each datapoint x in X, return the index of the leaf x\n",
            " |          ends up in. Leaves are numbered within\n",
            " |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
            " |          numbering.\n",
            " |  \n",
            " |  cost_complexity_pruning_path(self, X, y, sample_weight=None)\n",
            " |      Compute the pruning path during Minimal Cost-Complexity Pruning.\n",
            " |      \n",
            " |      See :ref:`minimal_cost_complexity_pruning` for details on the pruning\n",
            " |      process.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The training input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csc_matrix``.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          The target values (class labels) as integers or strings.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights. If None, then samples are equally weighted. Splits\n",
            " |          that would create child nodes with net zero or negative weight are\n",
            " |          ignored while searching for a split in each node. Splits are also\n",
            " |          ignored if they would result in any single class carrying a\n",
            " |          negative weight in either child node.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      ccp_path : Bunch\n",
            " |          Dictionary-like object, with attributes:\n",
            " |      \n",
            " |          ccp_alphas : ndarray\n",
            " |              Effective alphas of subtree during pruning.\n",
            " |      \n",
            " |          impurities : ndarray\n",
            " |              Sum of the impurities of the subtree leaves for the\n",
            " |              corresponding alpha value in ``ccp_alphas``.\n",
            " |  \n",
            " |  decision_path(self, X, check_input=True)\n",
            " |      Return the decision path in the tree.\n",
            " |      \n",
            " |      .. versionadded:: 0.18\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      check_input : bool, default=True\n",
            " |          Allow to bypass several input checking.\n",
            " |          Don't use this parameter unless you know what you do.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
            " |          Return a node indicator CSR matrix where non zero elements\n",
            " |          indicates that the samples goes through the nodes.\n",
            " |  \n",
            " |  get_depth(self)\n",
            " |      Return the depth of the decision tree.\n",
            " |      \n",
            " |      The depth of a tree is the maximum distance between the root\n",
            " |      and any leaf.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self.tree_.max_depth : int\n",
            " |          The maximum depth of the tree.\n",
            " |  \n",
            " |  get_n_leaves(self)\n",
            " |      Return the number of leaves of the decision tree.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self.tree_.n_leaves : int\n",
            " |          Number of leaves.\n",
            " |  \n",
            " |  predict(self, X, check_input=True)\n",
            " |      Predict class or regression value for X.\n",
            " |      \n",
            " |      For a classification model, the predicted class for each sample in X is\n",
            " |      returned. For a regression model, the predicted value based on X is\n",
            " |      returned.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      check_input : bool, default=True\n",
            " |          Allow to bypass several input checking.\n",
            " |          Don't use this parameter unless you know what you do.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          The predicted classes, or the predict values.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from BaseDecisionTree:\n",
            " |  \n",
            " |  feature_importances_\n",
            " |      Return the feature importances.\n",
            " |      \n",
            " |      The importance of a feature is computed as the (normalized) total\n",
            " |      reduction of the criterion brought by that feature.\n",
            " |      It is also known as the Gini importance.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      feature_importances_ : ndarray of shape (n_features,)\n",
            " |          Normalized total reduction of criteria by feature\n",
            " |          (Gini importance).\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : mapping of string to any\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as pipelines). The latter have parameters of the form\n",
            " |      ``<component>__<parameter>`` so that it's possible to update each\n",
            " |      component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Estimator instance.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPT9CKOM-_Os",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d0988956-6c50-48dd-9891-021169c98b1f"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "help(LinearRegression)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class LinearRegression in module sklearn.linear_model._base:\n",
            "\n",
            "class LinearRegression(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, LinearModel)\n",
            " |  Ordinary least squares Linear Regression.\n",
            " |  \n",
            " |  LinearRegression fits a linear model with coefficients w = (w1, ..., wp)\n",
            " |  to minimize the residual sum of squares between the observed targets in\n",
            " |  the dataset, and the targets predicted by the linear approximation.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  fit_intercept : bool, optional, default True\n",
            " |      Whether to calculate the intercept for this model. If set\n",
            " |      to False, no intercept will be used in calculations\n",
            " |      (i.e. data is expected to be centered).\n",
            " |  \n",
            " |  normalize : bool, optional, default False\n",
            " |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
            " |      If True, the regressors X will be normalized before regression by\n",
            " |      subtracting the mean and dividing by the l2-norm.\n",
            " |      If you wish to standardize, please use\n",
            " |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on\n",
            " |      an estimator with ``normalize=False``.\n",
            " |  \n",
            " |  copy_X : bool, optional, default True\n",
            " |      If True, X will be copied; else, it may be overwritten.\n",
            " |  \n",
            " |  n_jobs : int or None, optional (default=None)\n",
            " |      The number of jobs to use for the computation. This will only provide\n",
            " |      speedup for n_targets > 1 and sufficient large problems.\n",
            " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
            " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
            " |      for more details.\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  coef_ : array of shape (n_features, ) or (n_targets, n_features)\n",
            " |      Estimated coefficients for the linear regression problem.\n",
            " |      If multiple targets are passed during the fit (y 2D), this\n",
            " |      is a 2D array of shape (n_targets, n_features), while if only\n",
            " |      one target is passed, this is a 1D array of length n_features.\n",
            " |  \n",
            " |  rank_ : int\n",
            " |      Rank of matrix `X`. Only available when `X` is dense.\n",
            " |  \n",
            " |  singular_ : array of shape (min(X, y),)\n",
            " |      Singular values of `X`. Only available when `X` is dense.\n",
            " |  \n",
            " |  intercept_ : float or array of shape of (n_targets,)\n",
            " |      Independent term in the linear model. Set to 0.0 if\n",
            " |      `fit_intercept = False`.\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  sklearn.linear_model.Ridge : Ridge regression addresses some of the\n",
            " |      problems of Ordinary Least Squares by imposing a penalty on the\n",
            " |      size of the coefficients with l2 regularization.\n",
            " |  sklearn.linear_model.Lasso : The Lasso is a linear model that estimates\n",
            " |      sparse coefficients with l1 regularization.\n",
            " |  sklearn.linear_model.ElasticNet : Elastic-Net is a linear regression\n",
            " |      model trained with both l1 and l2 -norm regularization of the\n",
            " |      coefficients.\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  From the implementation point of view, this is just plain Ordinary\n",
            " |  Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> import numpy as np\n",
            " |  >>> from sklearn.linear_model import LinearRegression\n",
            " |  >>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
            " |  >>> # y = 1 * x_0 + 2 * x_1 + 3\n",
            " |  >>> y = np.dot(X, np.array([1, 2])) + 3\n",
            " |  >>> reg = LinearRegression().fit(X, y)\n",
            " |  >>> reg.score(X, y)\n",
            " |  1.0\n",
            " |  >>> reg.coef_\n",
            " |  array([1., 2.])\n",
            " |  >>> reg.intercept_\n",
            " |  3.0000...\n",
            " |  >>> reg.predict(np.array([[3, 5]]))\n",
            " |  array([16.])\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      LinearRegression\n",
            " |      sklearn.base.MultiOutputMixin\n",
            " |      sklearn.base.RegressorMixin\n",
            " |      LinearModel\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, X, y, sample_weight=None)\n",
            " |      Fit linear model.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          Training data\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
            " |          Target values. Will be cast to X's dtype if necessary\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Individual weights for each sample\n",
            " |      \n",
            " |          .. versionadded:: 0.17\n",
            " |             parameter *sample_weight* support to LinearRegression.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : returns an instance of self.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.RegressorMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the coefficient of determination R^2 of the prediction.\n",
            " |      \n",
            " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
            " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
            " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
            " |      The best possible score is 1.0 and it can be negative (because the\n",
            " |      model can be arbitrarily worse). A constant model that always\n",
            " |      predicts the expected value of y, disregarding the input features,\n",
            " |      would get a R^2 score of 0.0.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples. For some estimators this may be a\n",
            " |          precomputed kernel matrix or a list of generic objects instead,\n",
            " |          shape = (n_samples, n_samples_fitted),\n",
            " |          where n_samples_fitted is the number of\n",
            " |          samples used in the fitting for the estimator.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True values for X.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          R^2 of self.predict(X) wrt. y.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      The R2 score used when calling ``score`` on a regressor will use\n",
            " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
            " |      with :func:`~sklearn.metrics.r2_score`. This will influence the\n",
            " |      ``score`` method of all the multioutput regressors (except for\n",
            " |      :class:`~sklearn.multioutput.MultiOutputRegressor`). To specify the\n",
            " |      default value manually and avoid the warning, please either call\n",
            " |      :func:`~sklearn.metrics.r2_score` directly or make a custom scorer with\n",
            " |      :func:`~sklearn.metrics.make_scorer` (the built-in scorer ``'r2'`` uses\n",
            " |      ``multioutput='uniform_average'``).\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from LinearModel:\n",
            " |  \n",
            " |  predict(self, X)\n",
            " |      Predict using the linear model.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
            " |          Samples.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      C : array, shape (n_samples,)\n",
            " |          Returns predicted values.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : mapping of string to any\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as pipelines). The latter have parameters of the form\n",
            " |      ``<component>__<parameter>`` so that it's possible to update each\n",
            " |      component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Estimator instance.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESUCAKEi_Iz4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "49e1967d-8c2f-41aa-8acc-30849e0e2056"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "help(KMeans)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class KMeans in module sklearn.cluster._kmeans:\n",
            "\n",
            "class KMeans(sklearn.base.TransformerMixin, sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
            " |  K-Means clustering.\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <k_means>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  \n",
            " |  n_clusters : int, default=8\n",
            " |      The number of clusters to form as well as the number of\n",
            " |      centroids to generate.\n",
            " |  \n",
            " |  init : {'k-means++', 'random'} or ndarray of shape             (n_clusters, n_features), default='k-means++'\n",
            " |      Method for initialization, defaults to 'k-means++':\n",
            " |  \n",
            " |      'k-means++' : selects initial cluster centers for k-mean\n",
            " |      clustering in a smart way to speed up convergence. See section\n",
            " |      Notes in k_init for more details.\n",
            " |  \n",
            " |      'random': choose k observations (rows) at random from data for\n",
            " |      the initial centroids.\n",
            " |  \n",
            " |      If an ndarray is passed, it should be of shape (n_clusters, n_features)\n",
            " |      and gives the initial centers.\n",
            " |  \n",
            " |  n_init : int, default=10\n",
            " |      Number of time the k-means algorithm will be run with different\n",
            " |      centroid seeds. The final results will be the best output of\n",
            " |      n_init consecutive runs in terms of inertia.\n",
            " |  \n",
            " |  max_iter : int, default=300\n",
            " |      Maximum number of iterations of the k-means algorithm for a\n",
            " |      single run.\n",
            " |  \n",
            " |  tol : float, default=1e-4\n",
            " |      Relative tolerance with regards to inertia to declare convergence.\n",
            " |  \n",
            " |  precompute_distances : 'auto' or bool, default='auto'\n",
            " |      Precompute distances (faster but takes more memory).\n",
            " |  \n",
            " |      'auto' : do not precompute distances if n_samples * n_clusters > 12\n",
            " |      million. This corresponds to about 100MB overhead per job using\n",
            " |      double precision.\n",
            " |  \n",
            " |      True : always precompute distances.\n",
            " |  \n",
            " |      False : never precompute distances.\n",
            " |  \n",
            " |  verbose : int, default=0\n",
            " |      Verbosity mode.\n",
            " |  \n",
            " |  random_state : int, RandomState instance, default=None\n",
            " |      Determines random number generation for centroid initialization. Use\n",
            " |      an int to make the randomness deterministic.\n",
            " |      See :term:`Glossary <random_state>`.\n",
            " |  \n",
            " |  copy_x : bool, default=True\n",
            " |      When pre-computing distances it is more numerically accurate to center\n",
            " |      the data first.  If copy_x is True (default), then the original data is\n",
            " |      not modified, ensuring X is C-contiguous.  If False, the original data\n",
            " |      is modified, and put back before the function returns, but small\n",
            " |      numerical differences may be introduced by subtracting and then adding\n",
            " |      the data mean, in this case it will also not ensure that data is\n",
            " |      C-contiguous which may cause a significant slowdown.\n",
            " |  \n",
            " |  n_jobs : int, default=None\n",
            " |      The number of jobs to use for the computation. This works by computing\n",
            " |      each of the n_init runs in parallel.\n",
            " |  \n",
            " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
            " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
            " |      for more details.\n",
            " |  \n",
            " |  algorithm : {\"auto\", \"full\", \"elkan\"}, default=\"auto\"\n",
            " |      K-means algorithm to use. The classical EM-style algorithm is \"full\".\n",
            " |      The \"elkan\" variation is more efficient by using the triangle\n",
            " |      inequality, but currently doesn't support sparse data. \"auto\" chooses\n",
            " |      \"elkan\" for dense data and \"full\" for sparse data.\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  cluster_centers_ : ndarray of shape (n_clusters, n_features)\n",
            " |      Coordinates of cluster centers. If the algorithm stops before fully\n",
            " |      converging (see ``tol`` and ``max_iter``), these will not be\n",
            " |      consistent with ``labels_``.\n",
            " |  \n",
            " |  labels_ : ndarray of shape (n_samples,)\n",
            " |      Labels of each point\n",
            " |  \n",
            " |  inertia_ : float\n",
            " |      Sum of squared distances of samples to their closest cluster center.\n",
            " |  \n",
            " |  n_iter_ : int\n",
            " |      Number of iterations run.\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  \n",
            " |  MiniBatchKMeans\n",
            " |      Alternative online implementation that does incremental updates\n",
            " |      of the centers positions using mini-batches.\n",
            " |      For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n",
            " |      probably much faster than the default batch implementation.\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  The k-means problem is solved using either Lloyd's or Elkan's algorithm.\n",
            " |  \n",
            " |  The average complexity is given by O(k n T), were n is the number of\n",
            " |  samples and T is the number of iteration.\n",
            " |  \n",
            " |  The worst case complexity is given by O(n^(k+2/p)) with\n",
            " |  n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii,\n",
            " |  'How slow is the k-means method?' SoCG2006)\n",
            " |  \n",
            " |  In practice, the k-means algorithm is very fast (one of the fastest\n",
            " |  clustering algorithms available), but it falls in local minima. That's why\n",
            " |  it can be useful to restart it several times.\n",
            " |  \n",
            " |  If the algorithm stops before fully converging (because of ``tol`` or\n",
            " |  ``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\n",
            " |  i.e. the ``cluster_centers_`` will not be the means of the points in each\n",
            " |  cluster. Also, the estimator will reassign ``labels_`` after the last\n",
            " |  iteration to make ``labels_`` consistent with ``predict`` on the training\n",
            " |  set.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  \n",
            " |  >>> from sklearn.cluster import KMeans\n",
            " |  >>> import numpy as np\n",
            " |  >>> X = np.array([[1, 2], [1, 4], [1, 0],\n",
            " |  ...               [10, 2], [10, 4], [10, 0]])\n",
            " |  >>> kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
            " |  >>> kmeans.labels_\n",
            " |  array([1, 1, 1, 0, 0, 0], dtype=int32)\n",
            " |  >>> kmeans.predict([[0, 0], [12, 3]])\n",
            " |  array([1, 0], dtype=int32)\n",
            " |  >>> kmeans.cluster_centers_\n",
            " |  array([[10.,  2.],\n",
            " |         [ 1.,  2.]])\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      KMeans\n",
            " |      sklearn.base.TransformerMixin\n",
            " |      sklearn.base.ClusterMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=None, algorithm='auto')\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, X, y=None, sample_weight=None)\n",
            " |      Compute k-means clustering.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
            " |          Training instances to cluster. It must be noted that the data\n",
            " |          will be converted to C ordering, which will cause a memory\n",
            " |          copy if the given data is not C-contiguous.\n",
            " |      \n",
            " |      y : Ignored\n",
            " |          Not used, present here for API consistency by convention.\n",
            " |      \n",
            " |      sample_weight : array-like, shape (n_samples,), optional\n",
            " |          The weights for each observation in X. If None, all observations\n",
            " |          are assigned equal weight (default: None).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self\n",
            " |          Fitted estimator.\n",
            " |  \n",
            " |  fit_predict(self, X, y=None, sample_weight=None)\n",
            " |      Compute cluster centers and predict cluster index for each sample.\n",
            " |      \n",
            " |      Convenience method; equivalent to calling fit(X) followed by\n",
            " |      predict(X).\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          New data to transform.\n",
            " |      \n",
            " |      y : Ignored\n",
            " |          Not used, present here for API consistency by convention.\n",
            " |      \n",
            " |      sample_weight : array-like, shape (n_samples,), optional\n",
            " |          The weights for each observation in X. If None, all observations\n",
            " |          are assigned equal weight (default: None).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      labels : array, shape [n_samples,]\n",
            " |          Index of the cluster each sample belongs to.\n",
            " |  \n",
            " |  fit_transform(self, X, y=None, sample_weight=None)\n",
            " |      Compute clustering and transform X to cluster-distance space.\n",
            " |      \n",
            " |      Equivalent to fit(X).transform(X), but more efficiently implemented.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          New data to transform.\n",
            " |      \n",
            " |      y : Ignored\n",
            " |          Not used, present here for API consistency by convention.\n",
            " |      \n",
            " |      sample_weight : array-like, shape (n_samples,), optional\n",
            " |          The weights for each observation in X. If None, all observations\n",
            " |          are assigned equal weight (default: None).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X_new : array, shape [n_samples, k]\n",
            " |          X transformed in the new space.\n",
            " |  \n",
            " |  predict(self, X, sample_weight=None)\n",
            " |      Predict the closest cluster each sample in X belongs to.\n",
            " |      \n",
            " |      In the vector quantization literature, `cluster_centers_` is called\n",
            " |      the code book and each value returned by `predict` is the index of\n",
            " |      the closest code in the code book.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          New data to predict.\n",
            " |      \n",
            " |      sample_weight : array-like, shape (n_samples,), optional\n",
            " |          The weights for each observation in X. If None, all observations\n",
            " |          are assigned equal weight (default: None).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      labels : array, shape [n_samples,]\n",
            " |          Index of the cluster each sample belongs to.\n",
            " |  \n",
            " |  score(self, X, y=None, sample_weight=None)\n",
            " |      Opposite of the value of X on the K-means objective.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          New data.\n",
            " |      \n",
            " |      y : Ignored\n",
            " |          Not used, present here for API consistency by convention.\n",
            " |      \n",
            " |      sample_weight : array-like, shape (n_samples,), optional\n",
            " |          The weights for each observation in X. If None, all observations\n",
            " |          are assigned equal weight (default: None).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          Opposite of the value of X on the K-means objective.\n",
            " |  \n",
            " |  transform(self, X)\n",
            " |      Transform X to a cluster-distance space.\n",
            " |      \n",
            " |      In the new space, each dimension is the distance to the cluster\n",
            " |      centers.  Note that even if X is sparse, the array returned by\n",
            " |      `transform` will typically be dense.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          New data to transform.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X_new : array, shape [n_samples, k]\n",
            " |          X transformed in the new space.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : mapping of string to any\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as pipelines). The latter have parameters of the form\n",
            " |      ``<component>__<parameter>`` so that it's possible to update each\n",
            " |      component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Estimator instance.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjYuzi2E_c9l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d009bc30-74ca-4461-a1bf-bbb9236b8eb4"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "help(train_test_split)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on function train_test_split in module sklearn.model_selection._split:\n",
            "\n",
            "train_test_split(*arrays, **options)\n",
            "    Split arrays or matrices into random train and test subsets\n",
            "    \n",
            "    Quick utility that wraps input validation and\n",
            "    ``next(ShuffleSplit().split(X, y))`` and application to input data\n",
            "    into a single call for splitting (and optionally subsampling) data in a\n",
            "    oneliner.\n",
            "    \n",
            "    Read more in the :ref:`User Guide <cross_validation>`.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    *arrays : sequence of indexables with same length / shape[0]\n",
            "        Allowed inputs are lists, numpy arrays, scipy-sparse\n",
            "        matrices or pandas dataframes.\n",
            "    \n",
            "    test_size : float, int or None, optional (default=None)\n",
            "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
            "        of the dataset to include in the test split. If int, represents the\n",
            "        absolute number of test samples. If None, the value is set to the\n",
            "        complement of the train size. If ``train_size`` is also None, it will\n",
            "        be set to 0.25.\n",
            "    \n",
            "    train_size : float, int, or None, (default=None)\n",
            "        If float, should be between 0.0 and 1.0 and represent the\n",
            "        proportion of the dataset to include in the train split. If\n",
            "        int, represents the absolute number of train samples. If None,\n",
            "        the value is automatically set to the complement of the test size.\n",
            "    \n",
            "    random_state : int, RandomState instance or None, optional (default=None)\n",
            "        If int, random_state is the seed used by the random number generator;\n",
            "        If RandomState instance, random_state is the random number generator;\n",
            "        If None, the random number generator is the RandomState instance used\n",
            "        by `np.random`.\n",
            "    \n",
            "    shuffle : boolean, optional (default=True)\n",
            "        Whether or not to shuffle the data before splitting. If shuffle=False\n",
            "        then stratify must be None.\n",
            "    \n",
            "    stratify : array-like or None (default=None)\n",
            "        If not None, data is split in a stratified fashion, using this as\n",
            "        the class labels.\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    splitting : list, length=2 * len(arrays)\n",
            "        List containing train-test split of inputs.\n",
            "    \n",
            "        .. versionadded:: 0.16\n",
            "            If the input is sparse, the output will be a\n",
            "            ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
            "            input type.\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> import numpy as np\n",
            "    >>> from sklearn.model_selection import train_test_split\n",
            "    >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
            "    >>> X\n",
            "    array([[0, 1],\n",
            "           [2, 3],\n",
            "           [4, 5],\n",
            "           [6, 7],\n",
            "           [8, 9]])\n",
            "    >>> list(y)\n",
            "    [0, 1, 2, 3, 4]\n",
            "    \n",
            "    >>> X_train, X_test, y_train, y_test = train_test_split(\n",
            "    ...     X, y, test_size=0.33, random_state=42)\n",
            "    ...\n",
            "    >>> X_train\n",
            "    array([[4, 5],\n",
            "           [0, 1],\n",
            "           [6, 7]])\n",
            "    >>> y_train\n",
            "    [2, 0, 3]\n",
            "    >>> X_test\n",
            "    array([[2, 3],\n",
            "           [8, 9]])\n",
            "    >>> y_test\n",
            "    [1, 4]\n",
            "    \n",
            "    >>> train_test_split(y, shuffle=False)\n",
            "    [[0, 1, 2], [3, 4]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXzHuNO2_ziZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c4f7e47e-812e-495f-fba9-86c8e5e875e5"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "help(classification_report)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on function classification_report in module sklearn.metrics._classification:\n",
            "\n",
            "classification_report(y_true, y_pred, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False, zero_division='warn')\n",
            "    Build a text report showing the main classification metrics\n",
            "    \n",
            "    Read more in the :ref:`User Guide <classification_report>`.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    y_true : 1d array-like, or label indicator array / sparse matrix\n",
            "        Ground truth (correct) target values.\n",
            "    \n",
            "    y_pred : 1d array-like, or label indicator array / sparse matrix\n",
            "        Estimated targets as returned by a classifier.\n",
            "    \n",
            "    labels : array, shape = [n_labels]\n",
            "        Optional list of label indices to include in the report.\n",
            "    \n",
            "    target_names : list of strings\n",
            "        Optional display names matching the labels (same order).\n",
            "    \n",
            "    sample_weight : array-like of shape (n_samples,), default=None\n",
            "        Sample weights.\n",
            "    \n",
            "    digits : int\n",
            "        Number of digits for formatting output floating point values.\n",
            "        When ``output_dict`` is ``True``, this will be ignored and the\n",
            "        returned values will not be rounded.\n",
            "    \n",
            "    output_dict : bool (default = False)\n",
            "        If True, return output as dict\n",
            "    \n",
            "    zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
            "        Sets the value to return when there is a zero division. If set to\n",
            "        \"warn\", this acts as 0, but warnings are also raised.\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    report : string / dict\n",
            "        Text summary of the precision, recall, F1 score for each class.\n",
            "        Dictionary returned if output_dict is True. Dictionary has the\n",
            "        following structure::\n",
            "    \n",
            "            {'label 1': {'precision':0.5,\n",
            "                         'recall':1.0,\n",
            "                         'f1-score':0.67,\n",
            "                         'support':1},\n",
            "             'label 2': { ... },\n",
            "              ...\n",
            "            }\n",
            "    \n",
            "        The reported averages include macro average (averaging the unweighted\n",
            "        mean per label), weighted average (averaging the support-weighted mean\n",
            "        per label), and sample average (only for multilabel classification).\n",
            "        Micro average (averaging the total true positives, false negatives and\n",
            "        false positives) is only shown for multi-label or multi-class\n",
            "        with a subset of classes, because it corresponds to accuracy otherwise.\n",
            "        See also :func:`precision_recall_fscore_support` for more details\n",
            "        on averages.\n",
            "    \n",
            "        Note that in binary classification, recall of the positive class\n",
            "        is also known as \"sensitivity\"; recall of the negative class is\n",
            "        \"specificity\".\n",
            "    \n",
            "    See also\n",
            "    --------\n",
            "    precision_recall_fscore_support, confusion_matrix,\n",
            "    multilabel_confusion_matrix\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> from sklearn.metrics import classification_report\n",
            "    >>> y_true = [0, 1, 2, 2, 2]\n",
            "    >>> y_pred = [0, 0, 2, 2, 1]\n",
            "    >>> target_names = ['class 0', 'class 1', 'class 2']\n",
            "    >>> print(classification_report(y_true, y_pred, target_names=target_names))\n",
            "                  precision    recall  f1-score   support\n",
            "    <BLANKLINE>\n",
            "         class 0       0.50      1.00      0.67         1\n",
            "         class 1       0.00      0.00      0.00         1\n",
            "         class 2       1.00      0.67      0.80         3\n",
            "    <BLANKLINE>\n",
            "        accuracy                           0.60         5\n",
            "       macro avg       0.50      0.56      0.49         5\n",
            "    weighted avg       0.70      0.60      0.61         5\n",
            "    <BLANKLINE>\n",
            "    >>> y_pred = [1, 1, 0]\n",
            "    >>> y_true = [1, 1, 1]\n",
            "    >>> print(classification_report(y_true, y_pred, labels=[1, 2, 3]))\n",
            "                  precision    recall  f1-score   support\n",
            "    <BLANKLINE>\n",
            "               1       1.00      0.67      0.80         3\n",
            "               2       0.00      0.00      0.00         0\n",
            "               3       0.00      0.00      0.00         0\n",
            "    <BLANKLINE>\n",
            "       micro avg       1.00      0.67      0.80         3\n",
            "       macro avg       0.33      0.22      0.27         3\n",
            "    weighted avg       1.00      0.67      0.80         3\n",
            "    <BLANKLINE>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl4vvbzfAKi7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c3a91d79-6ef2-47f2-836d-e8e1f22b545e"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "help(accuracy_score)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on function accuracy_score in module sklearn.metrics._classification:\n",
            "\n",
            "accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)\n",
            "    Accuracy classification score.\n",
            "    \n",
            "    In multilabel classification, this function computes subset accuracy:\n",
            "    the set of labels predicted for a sample must *exactly* match the\n",
            "    corresponding set of labels in y_true.\n",
            "    \n",
            "    Read more in the :ref:`User Guide <accuracy_score>`.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    y_true : 1d array-like, or label indicator array / sparse matrix\n",
            "        Ground truth (correct) labels.\n",
            "    \n",
            "    y_pred : 1d array-like, or label indicator array / sparse matrix\n",
            "        Predicted labels, as returned by a classifier.\n",
            "    \n",
            "    normalize : bool, optional (default=True)\n",
            "        If ``False``, return the number of correctly classified samples.\n",
            "        Otherwise, return the fraction of correctly classified samples.\n",
            "    \n",
            "    sample_weight : array-like of shape (n_samples,), default=None\n",
            "        Sample weights.\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    score : float\n",
            "        If ``normalize == True``, return the fraction of correctly\n",
            "        classified samples (float), else returns the number of correctly\n",
            "        classified samples (int).\n",
            "    \n",
            "        The best performance is 1 with ``normalize == True`` and the number\n",
            "        of samples with ``normalize == False``.\n",
            "    \n",
            "    See also\n",
            "    --------\n",
            "    jaccard_score, hamming_loss, zero_one_loss\n",
            "    \n",
            "    Notes\n",
            "    -----\n",
            "    In binary and multiclass classification, this function is equal\n",
            "    to the ``jaccard_score`` function.\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> from sklearn.metrics import accuracy_score\n",
            "    >>> y_pred = [0, 2, 1, 3]\n",
            "    >>> y_true = [0, 1, 2, 3]\n",
            "    >>> accuracy_score(y_true, y_pred)\n",
            "    0.5\n",
            "    >>> accuracy_score(y_true, y_pred, normalize=False)\n",
            "    2\n",
            "    \n",
            "    In the multilabel case with binary label indicators:\n",
            "    \n",
            "    >>> import numpy as np\n",
            "    >>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
            "    0.5\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX2mE_fMAWmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}